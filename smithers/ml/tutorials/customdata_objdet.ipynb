{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to prepare your own dataset for training an Object Detector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gathering data\n",
    "First of all you need data. To train a robust classifier, we need a lot of pictures which should differ a lot from each other. So they should have different backgrounds, random object, and varying lighting conditions. You can either take the pictures yourself or you can download them from the internet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the Dataset folder - Pascal VOC notation\n",
    "In order to train an object detector using our own dataset, we need to construct an internal structure for the folder that contains our images ('./data'). In order to generate this we follow the Pascal VOC notation. Pascal VOC annotations are saved as XML files, one XML file per image. Each XML file contains the path to the image in the 'path' element, the bounding box stored in an 'object' element and other features as can be seen in the example below.\n",
    "\n",
    "<img src = \"images/pascalvoc.PNG\" style = \"height:400px\">\n",
    "\n",
    "As you can see the bounding box is defined by two points, the upper left and bottom right corners.\n",
    "\n",
    "We need to construct inside the main folder 'data' another subfolder 'VOC2007', that contains three subfolders:\n",
    "1. Annotations: Inside this folder we will put the Pascal VOC formatted annotation XML files, that we are going to generate below using LabelImg.\n",
    "2. ImageSets: Inside this folder there is another subfolder 'Main', that contains two files 'test.txt' and 'trainval.txt'. In these two files all the images that belong to that category are listed.\n",
    "3. JPEGImages: Here there are all the images, that has to be in the JPG format.\n",
    "\n",
    "At the end your dataset should have a structure like this:\n",
    "\n",
    "![](images/structure.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Data\n",
    "In order to label our data, the image labeling software LabelImg was used. It is available on Github https://github.com/tzutalin/labelImg and thanks to this tool we can create easily the xml files with a similar structure to the PASCAL VOC dataset for all images in the training and testing directory.\n",
    "### Instruction to install labelImg\n",
    "1. git clone https://github.com/tzutalin/labelImg\n",
    "2. conda install pyqt=5\n",
    "3. pyrcc5 -o libs/resources.py resources.qrc\n",
    "4. python labelImg.py\n",
    "### Steps\n",
    "1. Build and launch LabelImg using the instructions above\n",
    "2. In the left column choose the saved annotation (Pascal VOC / Yolo). Make sure Pascal VOC is selected.\n",
    "3. Click 'Open Dir' to open the directory that contains all your images (the folder should be found following this path \"./data/VOCdevkit/VOC2007/ImageSets\")\n",
    "4. Change save directory for the XML annotation files to \"./data/VOCdevkit/VOC2007/Annotations\".\n",
    "5. Click 'Create RectBox' \n",
    "6. Click and release left mouse to select a region to annotate the rect box\n",
    "7. You can use right mouse to drag the rect box to copy or move it\n",
    "\n",
    "\n",
    "A txt file with the different classes used in the labeling is also created and placed in the folders you are working on. \n",
    "\n",
    "<img src=\"images/labelimg.jpg\" style=\"height:500px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform image resolution\n",
    "Usually the images you collect will have different sizes and high resolution, since you can take them using different \n",
    "photographic equipment: a mobilephone, a webcam, a reflex camera, etc. Thus, it is needed to transform all the images to a lower scale (the same for all the pictures) in order to speed up the training (e.g. 200 x 150 can be an option, but it is important to check, while changing the resolution, that you are able to distinguish and recognize the objects in the picture, otherwise it will be difficult also for the CNN to learn the main features of your objects). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def rescale_images(directory, size):\n",
    "    for img in os.listdir(directory):\n",
    "        im = Image.open(directory + '/' + img)\n",
    "        im_resized = im.resize(size, Image.ANTIALIAS)\n",
    "        im_resized.save(directory + '/' + img)\n",
    "        \n",
    "path_images = '../data_lab/VOC2007/JPEGImages'\n",
    "WIDTH_NEW = 800\n",
    "HEIGHT_NEW = 600\n",
    "rescale_images(path_images, (WIDTH_NEW, HEIGHT_NEW))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Split of the dataset\n",
    "Once we have our images, we need to split them between those we use for the train, 80% of them, and those for the test, the remaining 20%. First of all we need to create a txt file, 'datafile.txt', that contains the name of all the images in our dataset. Then, using the function sklearn.model_selection.train_test_split we can make this split of the dataset and create the two txt files to place inside the folder 'ImageSets/Main', one with the train images and one with the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_file = '../data_lab/VOC2007/ImageSets/Main/trainval.txt'\n",
    "test_file = '../data_lab/VOC2007/ImageSets/Main/test.txt'\n",
    "with open(\"../data_lab/datafile.txt\", \"r\") as f:\n",
    " # in Windows you may need to put rb instead of r mode \n",
    "   data = f.read().split('\\n')\n",
    "   data = numpy.array(data)  #convert array to numpy type array\n",
    "\n",
    "   train, test = train_test_split(data,test_size=0.2)      \n",
    "   split = [train, test] \n",
    "   # the ouputs here are two lists containing train-test split of inputs.\n",
    "   lengths = [len(train), len(test)]\n",
    "   out_train = open(train_file,\"w\")\n",
    "   out_test = open(test_file, \"w\")\n",
    "   out_file = [out_train, out_test]\n",
    "   out = 0\n",
    "   for l in lengths:\n",
    "        for i in range(l):\n",
    "            name_img = split[out][i]\n",
    "            out_file[out].write(name_img + '\\n')\n",
    "        out_file[out].close()    \n",
    "        out += 1\n",
    "        \n",
    "        \n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "# split into a training and testing set\n",
    "# y here the label associated\n",
    "#X_train, X_test, y_train, y_test = train_test_split(\n",
    "#    X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change xml file\n",
    "Once we have labeled and created all the xml files for our images, we could want to change the resolution of our images as explained above. In order to change the xml file every time we transform image resolution, it is necessary to define a function that modify only the lines corresponding at the features of the images we are changing (i.e. the width and height, the box position and also the path where they are located)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sys import argv\n",
    "from os import listdir, path\n",
    "import re\n",
    "\n",
    "\n",
    "WIDTH_NEW = 800\n",
    "HEIGHT_NEW = 600\n",
    "\n",
    "DIMLINE_MASK = r'<(?P<type1>width|height)>(?P<size>\\d+)</(?P<type2>width|height)>'\n",
    "BBLINE_MASK = r'<(?P<type1>xmin|xmax|ymin|ymax)>(?P<size>\\d+)</(?P<type2>xmin|xmax|ymin|ymax)>'\n",
    "NAMELINE_MASK = r'<(?P<type1>filename)>(?P<size>\\S+)</(?P<type2>filename)>'\n",
    "PATHLINE_MASK = r'<(?P<type1>path)>(?P<size>.+)</(?P<type2>path)>'\n",
    "#regular expression\n",
    "\n",
    "def resize_file(file_lines):\n",
    "    new_lines = []\n",
    "    for line in file_lines:\n",
    "        match = re.search(DIMLINE_MASK, line) or re.search(BBLINE_MASK, line) or  re.search(NAMELINE_MASK, line) or re.search(PATHLINE_MASK, line) \n",
    "        if match is not None:\n",
    "            size = match.group('size')\n",
    "            type1 = match.group('type1')\n",
    "            type2 = match.group('type2')    \n",
    "            if type1 != type2:\n",
    "                raise ValueError('Malformed line: {}'.format(line))\n",
    "          \n",
    "            if type1.startswith('f'):\n",
    "                new_name = size[:-3] + 'jpg'\n",
    "                new_line = '\\t<{}>{}</{}>\\n'.format(type1, new_name, type1)\n",
    "            elif type1.startswith('p'):\n",
    "                new_size = '/scratch/lmeneghe/electrolux/Object_Detector/data_lab/VOC2007/Annotations/' + new_name\n",
    "                new_line = '\\t<{}>{}</{}>\\n'.format(type1, new_size, type1)\n",
    "            elif type1.startswith('x'):\n",
    "                size = int(size)\n",
    "                new_size = int(round(size * WIDTH_NEW / width_old))\n",
    "                new_line = '\\t\\t\\t<{}>{}</{}>\\n'.format(type1, new_size, type1)\n",
    "            elif type1.startswith('y'):\n",
    "                size = int(size)\n",
    "                new_size = int(round(size * HEIGHT_NEW / height_old))\n",
    "                new_line = '\\t\\t\\t<{}>{}</{}>\\n'.format(type1, new_size, type1)\n",
    "            elif type1.startswith('w'):\n",
    "                size = int(size)\n",
    "                width_old = size\n",
    "                new_size = int(WIDTH_NEW)\n",
    "                new_line = '\\t\\t<{}>{}</{}>\\n'.format(type1, new_size, type1)\n",
    "            elif type1.startswith('h'):\n",
    "                size = int(size)\n",
    "                height_old = size\n",
    "                new_size = int(HEIGHT_NEW)\n",
    "                new_line = '\\t\\t<{}>{}</{}>\\n'.format(type1, new_size, type1)\n",
    "            else:\n",
    "                raise ValueError('Unknown type: {}'.format(type1))\n",
    "            #new_line = '\\t\\t\\t<{}>{}</{}>\\n'.format(type1, new_size, type1)\n",
    "            new_lines.append(new_line)\n",
    "        else:\n",
    "            new_lines.append(line)\n",
    "\n",
    "    return ''.join(new_lines)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "def change_xml(nome_file):\n",
    "    if len(nome_file) < 1:\n",
    "        raise ValueError('No file submitted')\n",
    "\n",
    "    if path.isdir(nome_file):\n",
    "    # the argument is a directory\n",
    "        files = listdir(nome_file)\n",
    "        for file in files:\n",
    "            file_path = path.join(nome_file, file)\n",
    "            file_name, file_ext = path.splitext(file)\n",
    "            #print(file_path, end='') # Questo non e` tanto astuto\n",
    "            if file_ext.lower() == '.xml':\n",
    "                #print(': CONVERTIMIIII!!!', end='')\n",
    "                with open(file_path,'r') as f:\n",
    "                        righe = f.readlines()\n",
    "\n",
    "                nuovo_file = resize_file(righe)\n",
    "                #print(nuovo_file)\n",
    "                with open(file_path,'w') as f:\n",
    "                    f.write(nuovo_file)\n",
    "            #print()\n",
    "        \n",
    "    else:\n",
    "        # otherwise i have a file (hopefully)\n",
    "        with open(nome_file,'r') as f:\n",
    "            righe = f.readlines()\n",
    "\n",
    "        nuovo_file = resize_file(righe)\n",
    "        #print(nuovo_file)\n",
    "        with open(nome_file,'w') as f:\n",
    "            f.write(nuovo_file)        \n",
    "\n",
    "#insert name of the xml file or directory that contains them\n",
    "xml_file = '../data_lab/VOC2007/Annotations' \n",
    "change_xml(xml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optional: How to convert your dataset in the COCO notation\n",
    "You are out of luck if your object detection training pipeline require COCO data format since the labeling tool we use does not support COCO annotation format. If you already have the dataset generated using Pascal VOC notation, you can then convert your annotation to COCO format.\n",
    "### COCO notation\n",
    "For the COCO data format, first of all, there is only a single JSON file for all the annotation in a dataset or one for each split of datasets(Train/Val/Test). The bounding box is express as the upper left starting coordinate and the box width and height, like \"bbox\" :[x,y,width,height]. Here is an example for the COCO data format JSON file which just contains one image as seen the top-level “images” element, 3 unique categories/classes in total seen in top-level “categories” element and 2 annotated bounding boxes for the image seen in top-level “annotations” element. If you want to understand better the COCO format, check the official webpage of the COCO dataset: http://cocodataset.org/#format-data.\n",
    "\n",
    "<img src=\"images/coco.PNG\" style=\"height:800px\">\n",
    "\n",
    "Once you have some annotated XML and images files with a folder structure similar to the one explained above, you can generate a COCO data formatted JSON file using the function voc2coco (see also https://github.com/Tony607/voc2coco).\n",
    "\n",
    "ATTENTION: In order to use this function the name of all the images must be numbers. So you need to rename all the images if this is not true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of xml files: 14\n",
      "Success: ./images/output.json\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "\n",
    "START_BOUNDING_BOX_ID = 1\n",
    "PRE_DEFINE_CATEGORIES = None\n",
    "# If necessary, pre-define category and its id\n",
    "#  PRE_DEFINE_CATEGORIES = {\"aeroplane\": 1, \"bicycle\": 2, \"bird\": 3, \"boat\": 4,\n",
    "#  \"bottle\":5, \"bus\": 6, \"car\": 7, \"cat\": 8, \"chair\": 9,\n",
    "#  \"cow\": 10, \"diningtable\": 11, \"dog\": 12, \"horse\": 13,\n",
    "#  \"motorbike\": 14, \"person\": 15, \"pottedplant\": 16,\n",
    "#  \"sheep\": 17, \"sofa\": 18, \"train\": 19, \"tvmonitor\": 20}\n",
    "\n",
    "\n",
    "def get(root, name):\n",
    "    vars = root.findall(name)\n",
    "    return vars\n",
    "\n",
    "\n",
    "def get_and_check(root, name, length):\n",
    "    vars = root.findall(name)\n",
    "    if len(vars) == 0:\n",
    "        raise ValueError(\"Can not find %s in %s.\" % (name, root.tag))\n",
    "    if length > 0 and len(vars) != length:\n",
    "        raise ValueError(\n",
    "            \"The size of %s is supposed to be %d, but is %d.\"\n",
    "            % (name, length, len(vars))\n",
    "        )\n",
    "    if length == 1:\n",
    "        vars = vars[0]\n",
    "    return vars\n",
    "\n",
    "\n",
    "def get_filename_as_int(filename):\n",
    "    try:\n",
    "        filename = filename.replace(\"\\\\\", \"/\")\n",
    "        filename = os.path.splitext(os.path.basename(filename))[0]\n",
    "        return int(filename)\n",
    "    except:\n",
    "        raise ValueError(\"Filename %s is supposed to be an integer.\" % (filename))\n",
    "\n",
    "\n",
    "def get_categories(xml_files):\n",
    "    \"\"\"Generate category name to id mapping from a list of xml files.\n",
    "    \n",
    "    Arguments:\n",
    "        xml_files {list} -- A list of xml file paths.\n",
    "    \n",
    "    Returns:\n",
    "        dict -- category name to id mapping.\n",
    "    \"\"\"\n",
    "    classes_names = []\n",
    "    for xml_file in xml_files:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall(\"object\"):\n",
    "            classes_names.append(member[0].text)\n",
    "    classes_names = list(set(classes_names))\n",
    "    classes_names.sort()\n",
    "    return {name: i for i, name in enumerate(classes_names)}\n",
    "\n",
    "\n",
    "def convert(xml_files, json_file):\n",
    "    json_dict = {\"images\": [], \"type\": \"instances\", \"annotations\": [], \"categories\": []}\n",
    "    if PRE_DEFINE_CATEGORIES is not None:\n",
    "        categories = PRE_DEFINE_CATEGORIES\n",
    "    else:\n",
    "        categories = get_categories(xml_files)\n",
    "    bnd_id = START_BOUNDING_BOX_ID\n",
    "    for xml_file in xml_files:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        path = get(root, \"path\")\n",
    "        if len(path) == 1:\n",
    "            filename = os.path.basename(path[0].text)\n",
    "        elif len(path) == 0:\n",
    "            filename = get_and_check(root, \"filename\", 1).text\n",
    "        else:\n",
    "            raise ValueError(\"%d paths found in %s\" % (len(path), xml_file))\n",
    "        ## The filename must be a number\n",
    "        image_id = get_filename_as_int(filename)\n",
    "        size = get_and_check(root, \"size\", 1)\n",
    "        width = int(get_and_check(size, \"width\", 1).text)\n",
    "        height = int(get_and_check(size, \"height\", 1).text)\n",
    "        image = {\n",
    "            \"file_name\": filename,\n",
    "            \"height\": height,\n",
    "            \"width\": width,\n",
    "            \"id\": image_id,\n",
    "        }\n",
    "        json_dict[\"images\"].append(image)\n",
    "        ## Currently we do not support segmentation.\n",
    "        #  segmented = get_and_check(root, 'segmented', 1).text\n",
    "        #  assert segmented == '0'\n",
    "        for obj in get(root, \"object\"):\n",
    "            category = get_and_check(obj, \"name\", 1).text\n",
    "            if category not in categories:\n",
    "                new_id = len(categories)\n",
    "                categories[category] = new_id\n",
    "            category_id = categories[category]\n",
    "            bndbox = get_and_check(obj, \"bndbox\", 1)\n",
    "            xmin = int(get_and_check(bndbox, \"xmin\", 1).text) - 1\n",
    "            ymin = int(get_and_check(bndbox, \"ymin\", 1).text) - 1\n",
    "            xmax = int(get_and_check(bndbox, \"xmax\", 1).text)\n",
    "            ymax = int(get_and_check(bndbox, \"ymax\", 1).text)\n",
    "            assert xmax > xmin\n",
    "            assert ymax > ymin\n",
    "            o_width = abs(xmax - xmin)\n",
    "            o_height = abs(ymax - ymin)\n",
    "            ann = {\n",
    "                \"area\": o_width * o_height,\n",
    "                \"iscrowd\": 0,\n",
    "                \"image_id\": image_id,\n",
    "                \"bbox\": [xmin, ymin, o_width, o_height],\n",
    "                \"category_id\": category_id,\n",
    "                \"id\": bnd_id,\n",
    "                \"ignore\": 0,\n",
    "                \"segmentation\": [],\n",
    "            }\n",
    "            json_dict[\"annotations\"].append(ann)\n",
    "            bnd_id = bnd_id + 1\n",
    "\n",
    "    for cate, cid in categories.items():\n",
    "        cat = {\"supercategory\": \"none\", \"id\": cid, \"name\": cate}\n",
    "        json_dict[\"categories\"].append(cat)\n",
    "\n",
    "    #os.makedirs(os.path.dirname(json_file), exist_ok=True)\n",
    "    json_fp = open(json_file, \"w\")\n",
    "    json_str = json.dumps(json_dict)\n",
    "    json_fp.write(json_str)\n",
    "    json_fp.close()\n",
    "\n",
    "\n",
    "xml_dir = './data/Annotations'\n",
    "json_file = './data/output.json'\n",
    "xml_files = glob.glob(os.path.join(xml_dir, \"*.xml\"))\n",
    "# If you want to do train/test split, you can pass a subset of xml files to convert function.\n",
    "print(\"Number of xml files: {}\".format(len(xml_files)))\n",
    "convert(xml_files, json_file)\n",
    "print(\"Success: {}\".format(json_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the COCO annotation\n",
    "Once we have the JSON file, we can visualize the COCO annotation by drawing bounding box and class labels as an overlay over the image. Open the COCO_Image_Viewer.ipynb in Jupyter notebook, that can be found on the GitHub page https://github.com/Tony607/voc2coco. \n",
    "http://cocodataset.org/#format-data"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c5bf16c94eb6f9341fa612a12f652937166e39821fa969ec7095b77ab48ffd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
